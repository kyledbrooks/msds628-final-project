{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sporting-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surprising-benefit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/jeopardy.csv')\n",
    "red_data = data.dropna()\n",
    "red_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moderate-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = {s:s.strip() for s in red_data.columns}\n",
    "red_data = red_data.rename(columns=new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "typical-frank",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>AirDate</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY  $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE  $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES  $200   \n",
       "\n",
       "                                            Question      Answer    AirDate  \\\n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus 2004-12-31   \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe 2004-12-31   \n",
       "2  The city of Yuma in this state has a record av...     Arizona 2004-12-31   \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's 2004-12-31   \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams 2004-12-31   \n",
       "\n",
       "   year  \n",
       "0  2004  \n",
       "1  2004  \n",
       "2  2004  \n",
       "3  2004  \n",
       "4  2004  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.rename(columns={' Question': 'question', ' Air Date': 'air_date', ' Round':'round'})\n",
    "red_data['AirDate'] = pd.to_datetime(red_data['Air Date'])\n",
    "red_data['year'] = pd.DatetimeIndex(red_data['Air Date']).year  # add year feature\n",
    "red_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "martial-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_list = red_data['Round'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minimal-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_data = red_data[red_data['Round'] == 'Jeopardy!']\n",
    "dj_data = red_data[red_data['Round'] == 'Double Jeopardy!']\n",
    "fj_data = red_data[red_data['Round'] == 'Final Jeopardy!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diverse-florida",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2004, 2010, 2000, 2006, 2009, 1996, 2007, 1997, 2002, 2003, 2001,\n",
       "       1990, 1999, 2008, 2011, 2005, 1998, 2012, 1992, 1986, 1985, 1991,\n",
       "       1993, 1994, 1988, 1987, 1995, 1984, 1989])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = red_data.year.unique()\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "female-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for roun in round_list[:-1]:\n",
    "    roun_data = red_data[red_data['Round'] == roun]\n",
    "    for year in years:\n",
    "        data_dict[f'{roun}-{year}'] = list(red_data.Question.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dietary-jason",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = list(data_dict.values())\n",
    "new_corpus = [(' ').join(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "instrumental-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-seattle",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "impossible-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing:\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# jeopardy_df['Text'] = jeopardy_df['Category'] + ' ' + jeopardy_df['Question'] + ' ' + jeopardy_df['Answer']\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.max_length = 3349390\n",
    "\n",
    "def get_noun_chunks(cell):\n",
    "    doc = nlp(cell)\n",
    "    res = []\n",
    "    for noun in doc.noun_chunks: # use np instead of np.text\n",
    "        res.append(str(noun))\n",
    "    return ' '.join(res)\n",
    "\n",
    "def normalize_text(cell):\n",
    "    # Remove punctuation:\n",
    "    cell = cell.lower()\n",
    "    cell = cell.translate(str.maketrans('', '', string.punctuation))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    token_list = word_tokenize(cell)\n",
    "    return ' '.join([wordnet_lemmatizer.lemmatize(w) for w in token_list if w not in stopwords.words('english')])\n",
    "\n",
    "\n",
    "# jeopardy_df['Text Nouns'] = jeopardy_df['Text'].apply(lambda x: get_noun_chunks(x)) # 13 minutes\n",
    "# jeopardy_df['Norm Text Nouns'] = jeopardy_df['Text Nouns'].apply(lambda x: normalize_text(x)) # 3 minutes\n",
    "\n",
    "\n",
    "# corpus = jeopardy_df[jeopardy_df['Year'] == '2001']['Norm Text Nouns'].values\n",
    "# vectorizer = TfidfVectorizer(stop_words={'english'})#, max_df=0.8)\n",
    "# X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-blond",
   "metadata": {},
   "source": [
    "## Jeopardy questions grouped by question value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "informed-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the possible values for jeopardy questions (excluding daily doubles)\n",
    "poss_values = ['$200', '$400', '$600', '$800', '$1000',  '$400', '$800', '$1200', '$1600', '$2000'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "critical-diesel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['$200', '$400', '$600', '$800', '$2,000', '$1000', '$100', '$300',\n",
       "       '$500', '$1,200', '$2,200', '$3,000', '$1,500', '$1,600', '$1,800',\n",
       "       '$1,000', '$1,400', '$700', '$4,800', '$2,500', '$1,300', '$900',\n",
       "       '$3,600', '$3,200', '$2,800', '$1,100', '$5,000', '$3,389',\n",
       "       '$5,600', '$3,800', '$5', '$2,600', '$2,300', '$1,263', '$2,400',\n",
       "       '$6,600', '$4,200', '$2,700', '$350', '$3,400', '$4,000', '$1,700',\n",
       "       '$250', '$3,989', '$2,100', '$750', '$1,900', '$3,100', '$5,200',\n",
       "       '$3,500', '$4,600', '$796', '$6,000', '$2,900'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_data.Value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "offensive-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_data = j_data[j_data['Value'].isin(poss_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j_data_val = j_data.groupby(by=['Value', 'Category']).agg({'Question': lambda x: ' '.join(x),\n",
    "#                                                            'Answer': lambda x: ' '.join(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beautiful-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_data_val = j_data.groupby(by=['Value']).agg({'Category': lambda x: ' '.join(x),\n",
    "                                               'Question': lambda x: ' '.join(x),\n",
    "                                               'Answer': lambda x: ' '.join(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prime-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$1000</th>\n",
       "      <td>HISTORY ESPN's TOP 10 ALL-TIME ATHLETES THE CO...</td>\n",
       "      <td>This Asian political party was founded in 1885...</td>\n",
       "      <td>the Congress Party (Wilt) Chamberlain K2 Ethan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$200</th>\n",
       "      <td>HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus Jim Thorpe Arizona McDonald's John ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$400</th>\n",
       "      <td>HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...</td>\n",
       "      <td>Built in 312 B.C. to link Rome &amp; the South of ...</td>\n",
       "      <td>the Appian Way Michael Jordan Washington Crate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$600</th>\n",
       "      <td>HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...</td>\n",
       "      <td>In 1000 Rajaraja I of the Cholas battled to ta...</td>\n",
       "      <td>Ceylon (or Sri Lanka) Jim Brown the UV index B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$800</th>\n",
       "      <td>HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...</td>\n",
       "      <td>Karl led the first of these Marxist organizati...</td>\n",
       "      <td>the International (Lou) Gehrig Morocco (Paul) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Category  \\\n",
       "Value                                                      \n",
       "$1000  HISTORY ESPN's TOP 10 ALL-TIME ATHLETES THE CO...   \n",
       "$200   HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...   \n",
       "$400   HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...   \n",
       "$600   HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...   \n",
       "$800   HISTORY ESPN's TOP 10 ALL-TIME ATHLETES EVERYB...   \n",
       "\n",
       "                                                Question  \\\n",
       "Value                                                      \n",
       "$1000  This Asian political party was founded in 1885...   \n",
       "$200   For the last 8 years of his life, Galileo was ...   \n",
       "$400   Built in 312 B.C. to link Rome & the South of ...   \n",
       "$600   In 1000 Rajaraja I of the Cholas battled to ta...   \n",
       "$800   Karl led the first of these Marxist organizati...   \n",
       "\n",
       "                                                  Answer  \n",
       "Value                                                     \n",
       "$1000  the Congress Party (Wilt) Chamberlain K2 Ethan...  \n",
       "$200   Copernicus Jim Thorpe Arizona McDonald's John ...  \n",
       "$400   the Appian Way Michael Jordan Washington Crate...  \n",
       "$600   Ceylon (or Sri Lanka) Jim Brown the UV index B...  \n",
       "$800   the International (Lou) Gehrig Morocco (Paul) ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_data_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "functioning-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_data_val['Text'] = j_data_val.Category + ' ' + j_data_val.Question + ' ' + j_data_val.Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sharp-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_text(cell):\n",
    "#     # Remove punctuation:\n",
    "#     cell = cell.lower()\n",
    "#     cell = cell.translate(str.maketrans('', '', string.punctuation))\n",
    "#     nlp = spacy.load('en')\n",
    "#     nlp.max_length = 1500000\n",
    "#     cell = nlp(cell).noun_chunks\n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     token_list = word_tokenize((' ').join(cell))\n",
    "    \n",
    "#     return ' '.join([wordnet_lemmatizer.lemmatize(w) for w in token_list if w not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "attached-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_data_val['Text Nouns'] = j_data_val['Text'].apply(lambda x: get_noun_chunks(x)) # 13 minutes\n",
    "j_data_val['Norm Text Nouns'] = j_data_val['Text Nouns'].apply(lambda x: normalize_text(x)) # 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j_data_val['Norm_Doc'] = j_data_val['Document'].apply(lambda x: normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-december",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j_data_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = jeopardy_df[jeopardy_df['Year'] == '2001']['Norm Text Nouns'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mediterranean-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [text.lower() for text in j_data_val['Norm Text Nouns'].values]\n",
    "# corpus = jeopardy_df[jeopardy_df['Year'] == '2001']['Norm Text Nouns'].values\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'})#, max_df=0.8)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words={'english'}, max_df=0.8)\n",
    "# X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "median-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = {index:word for index, word in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "favorite-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "asian-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, learning_method='online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "scientific-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylebrooks/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:806: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-1.0 * perword_bound)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_components=15)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "gross-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "decent-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "motivated-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "topics = defaultdict(list)\n",
    "for t in range(NUM_TOPICS):\n",
    "    # val > 0.5 -> it's part of the topic\n",
    "    for index, val in enumerate(word_topic[t].argsort()[-15:-1]):#[0,:]):\n",
    "        topics[t].append(Vocab[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "vanilla-power",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['oomf',\n",
       "              'triolas',\n",
       "              'acedeuce',\n",
       "              'mayhem',\n",
       "              'baloo',\n",
       "              'dubhe',\n",
       "              'benatar',\n",
       "              'lallemande',\n",
       "              'questionanswer',\n",
       "              'downward',\n",
       "              'emolument',\n",
       "              'media20050518j03ajpg',\n",
       "              'blemish',\n",
       "              'prussia'],\n",
       "             1: ['conversationalist',\n",
       "              'hrefhttpwwwjarchivecommedia20100719j24ajpg',\n",
       "              'amphissa',\n",
       "              'choroid',\n",
       "              'hrefhttpwwwjarchivecommedia20111115j05wmvplantinga',\n",
       "              'capsule',\n",
       "              'erharts',\n",
       "              'coruler',\n",
       "              'heavn',\n",
       "              'tienes',\n",
       "              'carolus',\n",
       "              'piccolo',\n",
       "              'bedin',\n",
       "              'snead'],\n",
       "             2: ['american',\n",
       "              'man',\n",
       "              'type',\n",
       "              'time',\n",
       "              'one',\n",
       "              'clue',\n",
       "              'world',\n",
       "              'country',\n",
       "              'new',\n",
       "              'first',\n",
       "              'hrefhttpwwwjarchivecom',\n",
       "              'word',\n",
       "              'city',\n",
       "              'state'],\n",
       "             3: ['oklahoman',\n",
       "              'stew',\n",
       "              'jaccuse',\n",
       "              'kansasnebraska',\n",
       "              'crecy',\n",
       "              'jetliner',\n",
       "              'genovian',\n",
       "              'appliance',\n",
       "              'nephrite',\n",
       "              'media20081215j02jpg',\n",
       "              'clearwater',\n",
       "              'hrefhttpwwwjarchivecommedia20070308j22jpg',\n",
       "              'synthpop',\n",
       "              '0702j21jpg'],\n",
       "             4: ['pecanbon',\n",
       "              'unselfish',\n",
       "              'begonia',\n",
       "              'vuitton',\n",
       "              'freefall',\n",
       "              'hrefhttpwwwjarchivecommedia20080407j04jpg',\n",
       "              'poolside',\n",
       "              'yorka',\n",
       "              'interstellar',\n",
       "              'triumphant',\n",
       "              'atlas',\n",
       "              'nagpur',\n",
       "              'partisan',\n",
       "              'starfruit'],\n",
       "             5: ['pillowcase',\n",
       "              'nano',\n",
       "              'cayce',\n",
       "              'reject',\n",
       "              'life',\n",
       "              'belost',\n",
       "              'addict',\n",
       "              'formfitting',\n",
       "              'shaanxi',\n",
       "              'selfrising',\n",
       "              'nasdaq',\n",
       "              '1700s',\n",
       "              'onlyi',\n",
       "              'skidoo'],\n",
       "             6: ['cephalopoda',\n",
       "              'hw',\n",
       "              'complicated',\n",
       "              'nuuk',\n",
       "              'staunch',\n",
       "              'postal',\n",
       "              'arawak',\n",
       "              'paa',\n",
       "              'tartini',\n",
       "              '10story',\n",
       "              'targetblankareaa',\n",
       "              'bryana',\n",
       "              'passive',\n",
       "              'hrefhttpwwwjarchivecommedia20100914j09ajpg'],\n",
       "             7: ['lear',\n",
       "              'eli',\n",
       "              'abu',\n",
       "              'gita',\n",
       "              'exocrine',\n",
       "              'cranky',\n",
       "              'steeple',\n",
       "              'mica',\n",
       "              'vantile',\n",
       "              'concede',\n",
       "              'nitroglycerin',\n",
       "              'said',\n",
       "              'ingo',\n",
       "              'mda'],\n",
       "             8: ['hrefhttpwwwjarchivecommedia20110523j03jpg',\n",
       "              'bhagavad',\n",
       "              'gynt',\n",
       "              'catalonia',\n",
       "              'moabites',\n",
       "              '825000',\n",
       "              'qcp',\n",
       "              'johnnie',\n",
       "              '500pound',\n",
       "              'alloy',\n",
       "              'boursin',\n",
       "              'handover',\n",
       "              'hormels',\n",
       "              'wie'],\n",
       "             9: ['marital',\n",
       "              'dizygotic',\n",
       "              'texasa',\n",
       "              'ichthyophobia',\n",
       "              'hrefhttpwwwjarchivecommedia20050916j22jpg',\n",
       "              'meitner',\n",
       "              'larrys',\n",
       "              'dentition',\n",
       "              'wackedout',\n",
       "              'boyd',\n",
       "              'kitty',\n",
       "              '0506j16jpg',\n",
       "              'ababa',\n",
       "              'innes'],\n",
       "             10: ['iota',\n",
       "              'aaaaa',\n",
       "              'washington',\n",
       "              'welltraveled',\n",
       "              'yearround',\n",
       "              'subeditor',\n",
       "              'compound',\n",
       "              'greatgrandfather',\n",
       "              'beancontaining',\n",
       "              'vexillology',\n",
       "              '2500',\n",
       "              'uh',\n",
       "              'ulaanbaatar',\n",
       "              'quoc'],\n",
       "             11: ['18thcentury',\n",
       "              'stanislavsky',\n",
       "              '378',\n",
       "              'hrefhttpwwwjarchivecommedia20061228j28jpg',\n",
       "              'guildenstern',\n",
       "              'titled',\n",
       "              'ole',\n",
       "              'state',\n",
       "              'hrefhttpwwwjarchivecommedia20051031j01jpg',\n",
       "              'interval',\n",
       "              'crichton',\n",
       "              'parisianborn',\n",
       "              'jackrabbit',\n",
       "              'completes'],\n",
       "             12: ['hrefhttpwwwjarchivecommedia20091119j10jpg',\n",
       "              'spoof',\n",
       "              'stone',\n",
       "              'pepsin',\n",
       "              'leatherheads',\n",
       "              'glitch',\n",
       "              'liberator',\n",
       "              'transplanting',\n",
       "              'lxxviii',\n",
       "              'schism',\n",
       "              'polonium',\n",
       "              'chaillu',\n",
       "              'jovi',\n",
       "              'dvorak'],\n",
       "             13: ['ganesha',\n",
       "              'crayolas',\n",
       "              'sauron',\n",
       "              'hrefhttpwwwjarchivecommedia20100429j24jpg',\n",
       "              'expert',\n",
       "              'deidre',\n",
       "              'hornsby',\n",
       "              'playtex',\n",
       "              '1711',\n",
       "              'roofed',\n",
       "              'couturier',\n",
       "              'michener',\n",
       "              'profound',\n",
       "              'anticommunists'],\n",
       "             14: ['fat',\n",
       "              'cytoplasm',\n",
       "              'glute',\n",
       "              'robe',\n",
       "              'messagea',\n",
       "              'put',\n",
       "              'caria',\n",
       "              'bromsgrove',\n",
       "              'experience',\n",
       "              'perlette',\n",
       "              'unlv',\n",
       "              'gritty',\n",
       "              'pulitzerwinning',\n",
       "              'gallimarie']})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_feature_names = vectorizer.get_feature_names()\n",
    "# plot_top_words(lda, tfidf_feature_names, 10,\n",
    "#                'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-filing",
   "metadata": {},
   "source": [
    "## Double Jeopardy questions grouped by question value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_data.Value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_values = ['$400', '$800', '$1200', '$1500', '$2000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_data = dj_data[dj_data['Value'].isin(dj_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_data_val = dj_data.groupby(by=['Value']).agg({'Category': lambda x: ' '.join(x),\n",
    "                                               'Question': lambda x: ' '.join(x),\n",
    "                                               'Answer': lambda x: ' '.join(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_data_val['Document'] = dj_data_val.Category + ' ' + dj_data_val.Question + ' ' + dj_data_val.Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_data_val['Norm_Doc'] = dj_data_val['Document'].apply(lambda x: normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [text.lower() for text in dj_data_val['Norm_Doc'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words={'english'}, max_df=0.8)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "Vocab = {index:word for index, word in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, learning_method='online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(X)\n",
    "word_topic = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "topics = defaultdict(list)\n",
    "for t in range(NUM_TOPICS):\n",
    "    # val > 0.5 -> it's part of the topic\n",
    "    for index, val in enumerate(word_topic[t].argsort()[-15:-1]):#[0,:]):\n",
    "        topics[t].append(Vocab[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_feature_names = vectorizer.get_feature_names()\n",
    "# plot_top_words(lda, tfidf_feature_names, 10,\n",
    "#                'Topics in LDA model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
